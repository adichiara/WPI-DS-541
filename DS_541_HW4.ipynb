{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS 541 HW4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAnz1a36vmkB"
      },
      "source": [
        "import numpy as np\r\n",
        "from numpy import eye as eye\r\n",
        "from numpy.linalg import inv as inv\r\n",
        "from numpy.linalg import solve as solve\r\n",
        "from numpy.linalg import eig as eig\r\n",
        "from numpy.linalg import matrix_power\r\n",
        "\r\n",
        "from numpy import random\r\n",
        "from numpy.random import randn\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import scipy.optimize\r\n",
        " \r\n",
        "import requests\r\n",
        "import urllib\r\n",
        "\r\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfaahjDcwEzH"
      },
      "source": [
        "\r\n",
        "# For this assignment, assume that every hidden layer has the same number of neurons.\r\n",
        "NUM_INPUT = 784\r\n",
        "NUM_OUTPUT = 10\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ2EN45Kw3iu"
      },
      "source": [
        "\r\n",
        "def load_data():\r\n",
        "    urllib.request.urlretrieve('https://s3.amazonaws.com/jrwprojects/fashion_mnist_train_images.npy', 'fashion_mnist_train_images.npy')\r\n",
        "    urllib.request.urlretrieve('https://s3.amazonaws.com/jrwprojects/fashion_mnist_test_images.npy', 'fashion_mnist_test_images.npy')\r\n",
        "    urllib.request.urlretrieve('https://s3.amazonaws.com/jrwprojects/fashion_mnist_train_labels.npy', 'fashion_mnist_train_labels.npy')\r\n",
        "    urllib.request.urlretrieve('https://s3.amazonaws.com/jrwprojects/fashion_mnist_test_labels.npy', 'fashion_mnist_test_labels.npy')\r\n",
        "\r\n",
        "    X_tr = np.reshape(np.load('fashion_mnist_train_images.npy'), (-1, 28*28))\r\n",
        "    X_te = np.reshape(np.load('fashion_mnist_test_images.npy'), (-1, 28*28))\r\n",
        "    y_tr = np.load('fashion_mnist_train_labels.npy')\r\n",
        "    y_te = np.load('fashion_mnist_test_labels.npy')\r\n",
        "\r\n",
        "    return ([X_tr, X_te, y_tr, y_te])\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lznert7rw6LO"
      },
      "source": [
        "\r\n",
        "def one_hot_encode_y(y_tr_scalar, y_te_scalar, max_num):\r\n",
        "    y_tr = (np.eye(max_num)[y_tr_scalar]).astype(int)\r\n",
        "    y_te = (np.eye(max_num)[y_te_scalar]).astype(int)\r\n",
        "    return ([y_tr, y_te])\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3TVsZ0ow6MZ"
      },
      "source": [
        "\r\n",
        "def normalize_inputs(X, max_val):\r\n",
        "    X = X/max_val\r\n",
        "    return X\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQDUxsjUfG64"
      },
      "source": [
        "\r\n",
        "# def forward_prop_with_output (x, y, weightsAndBiases):\r\n",
        "#     Ws, bs = unpack_new(weightsAndBiases)\r\n",
        "\r\n",
        "#     x = np.atleast_2d(x)\r\n",
        "#     y = np.atleast_2d(y)\r\n",
        "\r\n",
        "#     print(\"x:\", x.shape)\r\n",
        "#     print(\"y:\", y.shape)\r\n",
        "#     print()\r\n",
        "\r\n",
        "#     hs = []\r\n",
        "#     zs = []\r\n",
        "\r\n",
        "#     # set h0 equal to the input x.\r\n",
        "#     hs.append(x.T)\r\n",
        "#     # make dummy element for z[0] so indexing starts at 1\r\n",
        "#     zs.append(x.T)  \r\n",
        "#     # zs.append(np.zeros([hs[0].shape[0],hs[0].shape[1]]))\r\n",
        "\r\n",
        "#     print(\"Input layer\")\r\n",
        "#     print(\"k:\", 0)\r\n",
        "#     print(\"zs[\", 0, \"]  \", zs[0].shape, sep=\"\")\r\n",
        "#     print(\"hs[\", 0, \"]  \", hs[0].shape, sep=\"\")\r\n",
        "#     print()\r\n",
        "\r\n",
        "#     l = NUM_HIDDEN_LAYERS + 1\r\n",
        "\r\n",
        "#     print(\"Hidden layers\")    \r\n",
        "#     for k in range(1, l):\r\n",
        "#       b = np.atleast_2d(bs[k-1]).T\r\n",
        "#       zs.append(Ws[k-1].dot(hs[k-1]) + b) \r\n",
        "#       hs.append(ReLU(zs[k]))\r\n",
        "#       print(\"k:\", k)\r\n",
        "#       print(\"Ws[\", k-1, \"]  \", Ws[k-1].shape, sep=\"\")\r\n",
        "#       print(\"bs[\", k-1, \"]  \", bs[k-1].shape, sep=\"\")\r\n",
        "#       print(\"zs[\", k, \"]  \", zs[k].shape, sep=\"\")\r\n",
        "#       print(\"hs[\", k, \"]  \", hs[k].shape, sep=\"\")\r\n",
        "#       print()\r\n",
        "\r\n",
        "#     # output layer\r\n",
        "#     b = np.atleast_2d(bs[l-1]).T\r\n",
        "#     zs.append(Ws[l-1].dot(hs[l-1]) + b)   \r\n",
        "#     yhat = softmax(zs[l]) \r\n",
        "\r\n",
        "#     print(\"Output layer\")\r\n",
        "#     print(\"k:\", k)\r\n",
        "#     print(\"Ws[\", l-1, \"]  \", Ws[l-1].shape, sep=\"\")\r\n",
        "#     print(\"bs[\", l-1, \"]  \", bs[l-1].shape, sep=\"\")\r\n",
        "#     print(\"zs[\", l, \"]  \", zs[l].shape, sep=\"\")\r\n",
        "#     print(\"yhat\", yhat.shape)\r\n",
        "#     print()\r\n",
        "\r\n",
        "#     loss = fCE(y, yhat)\r\n",
        "#     print(\"fCE loss:\", loss)\r\n",
        "\r\n",
        "#     # Return loss, pre-activations, post-activations, and predictions\r\n",
        "#     return loss, zs, hs, yhat\r\n",
        "   "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D736CyT-dwxE"
      },
      "source": [
        "\r\n",
        "# def back_prop_with_output (x, y, weightsAndBiases):\r\n",
        "#     # dimensions:             indexes:\r\n",
        "#     # x: (m x n)\r\n",
        "#     # y: (c x n)\r\n",
        "#     # zs (outputs x n)        1 ... l\r\n",
        "#     # hs (outputs x n)        1 ... l \r\n",
        "#     # Ws (outputs x inputs)   0 ... l-1\r\n",
        "#     # bs (outputs)            0 ... l-1\r\n",
        "\r\n",
        "#     loss, zs, hs, yhat = forward_prop(x, y, weightsAndBiases)\r\n",
        "#     Ws, bs = unpack_new(weightsAndBiases)\r\n",
        "    \r\n",
        "#     x = np.atleast_2d(x)\r\n",
        "#     y = np.atleast_2d(y)\r\n",
        "\r\n",
        "#     y = y.T\r\n",
        "#     g = yhat - y\r\n",
        "\r\n",
        "#     n = y.shape[1]\r\n",
        "#     l = NUM_HIDDEN_LAYERS + 1\r\n",
        "\r\n",
        "#     dJdWs = []  # Gradients w.r.t. weights\r\n",
        "#     dJdbs = []  # Gradients w.r.t. biases\r\n",
        "\r\n",
        "#     print(\"NUM_HIDDEN_LAYERS:\", NUM_HIDDEN_LAYERS)\r\n",
        "#     print(\"l:\", l)\r\n",
        "#     print(\"y:\", y.shape)\r\n",
        "#     print(\"yhat:\", yhat.shape)\r\n",
        "#     print(\"g:\", g.shape)\r\n",
        "#     print()\r\n",
        "\r\n",
        "#     print(\"--------------------------\")\r\n",
        "#     print()\r\n",
        "\r\n",
        "#     for k in range(l, 0, -1):\r\n",
        "#       print(\"k:\", k)\r\n",
        "#       try:\r\n",
        "#         print(\"hs[\", k, \"]:  \", hs[k].shape, sep=\"\")\r\n",
        "#       except:\r\n",
        "#         pass\r\n",
        "#       try:\r\n",
        "#         print(\"hs[\", k-1, \"]:  \", hs[k-1].shape, sep=\"\")\r\n",
        "#       except:\r\n",
        "#         pass\r\n",
        "#       try:\r\n",
        "#         print(\"zs[\", k, \"]:  \", zs[k].shape, sep=\"\")\r\n",
        "#       except:\r\n",
        "#         pass\r\n",
        "#       try:\r\n",
        "#         print(\"zs[\", k-1, \"]:  \", zs[k-1].shape, sep=\"\")\r\n",
        "#       except:\r\n",
        "#         pass\r\n",
        "#       try:\r\n",
        "#         print(\"Ws[\", k-1, \"]:  \", Ws[k-1].shape, sep=\"\")\r\n",
        "#       except:\r\n",
        "#         pass\r\n",
        "#       try:\r\n",
        "#         print(\"bs[\", k-1, \"]:  \", bs[k-1].shape, sep=\"\")\r\n",
        "#       except:\r\n",
        "#         pass\r\n",
        "#       print(\"g:\", g.shape)\r\n",
        "\r\n",
        "#       dJdWs.append(g.dot(hs[k-1].T)/n)  # + reg grad       \r\n",
        "#       # dJdWs.append(hs[k-1].dot(g.T)/n)  # + reg grad       # old unpack \r\n",
        "#       dJdbs.append(np.mean(g, axis=1))\r\n",
        "\r\n",
        "#       print(\"dJdWs[\", k-1, \"]:  \", dJdWs[len(dJdWs)-1].shape, sep=\"\")\r\n",
        "#       print(\"dJdbs[\", k-1, \"]:  \", dJdbs[len(dJdbs)-1].shape, sep=\"\")\r\n",
        "#       print()\r\n",
        "      \r\n",
        "#       if (k > 1):\r\n",
        "#         g = Ws[k-1].T.dot(g)\r\n",
        "#         # g = Ws[k-1].dot(g)       # old unpack \r\n",
        "#         g = g * ReLU_prime(zs[k-1])\r\n",
        "\r\n",
        "#     print(\"--------------------------\")\r\n",
        "#     print()\r\n",
        "\r\n",
        "#     # reverse order\r\n",
        "#     dJdWs = dJdWs[::-1]\r\n",
        "#     dJdbs = dJdbs[::-1]\r\n",
        "#     print(\"dJdWs len:\", len(dJdWs))\r\n",
        "#     print(\"dJdbs len:\", len(dJdbs))\r\n",
        "\r\n",
        "#     # # Concatenate gradients\r\n",
        "#     return np.hstack([ dJdW.flatten() for dJdW in dJdWs ] + [ dJdb.flatten() for dJdb in dJdbs ]) \r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0gIgPctdpnu"
      },
      "source": [
        "\r\n",
        "def initWeightsAndBiases_new(NUM_HIDDEN_LAYERS, NUM_HIDDEN):\r\n",
        "    Ws = []\r\n",
        "    bs = []\r\n",
        "\r\n",
        "    np.random.seed(0)\r\n",
        "    W = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\r\n",
        "    Ws.append(W)\r\n",
        "    b = 0.01 * np.ones(NUM_HIDDEN)\r\n",
        "    bs.append(b)\r\n",
        "\r\n",
        "    for i in range(NUM_HIDDEN_LAYERS - 1):\r\n",
        "        W = 2*(np.random.random(size=(NUM_HIDDEN, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\r\n",
        "        Ws.append(W)\r\n",
        "        b = 0.01 * np.ones(NUM_HIDDEN)\r\n",
        "        bs.append(b)\r\n",
        "\r\n",
        "    W = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\r\n",
        "    Ws.append(W)\r\n",
        "    b = 0.01 * np.ones(NUM_OUTPUT)\r\n",
        "    bs.append(b)\r\n",
        "    return np.hstack([ W.flatten() for W in Ws ] + [ b.flatten() for b in bs ])\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGvQ_W6R-WB"
      },
      "source": [
        "\n",
        "def unpack_new (weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN):\n",
        "    # Unpack arguments\n",
        "    Ws = []\n",
        "\n",
        "    # Weight matrices\n",
        "    start = 0\n",
        "    end = NUM_INPUT*NUM_HIDDEN\n",
        "    W = weightsAndBiases[start:end]\n",
        "    Ws.append(W)\n",
        "\n",
        "    for i in range(NUM_HIDDEN_LAYERS - 1):\n",
        "        start = end\n",
        "        end = end + NUM_HIDDEN*NUM_HIDDEN\n",
        "        W = weightsAndBiases[start:end]\n",
        "        Ws.append(W)\n",
        "\n",
        "    start = end\n",
        "    end = end + NUM_HIDDEN*NUM_OUTPUT\n",
        "    W = weightsAndBiases[start:end]\n",
        "    Ws.append(W)\n",
        "\n",
        "    Ws[0] = Ws[0].reshape(NUM_HIDDEN, NUM_INPUT)\n",
        "    for i in range(1, NUM_HIDDEN_LAYERS):\n",
        "        # Convert from vectors into matrices\n",
        "        Ws[i] = Ws[i].reshape(NUM_HIDDEN, NUM_HIDDEN)\n",
        "    Ws[-1] = Ws[-1].reshape(NUM_OUTPUT, NUM_HIDDEN)\n",
        "\n",
        "    # Bias terms\n",
        "    bs = []\n",
        "    start = end\n",
        "    end = end + NUM_HIDDEN\n",
        "    b = weightsAndBiases[start:end]\n",
        "    bs.append(b)\n",
        "\n",
        "    for i in range(NUM_HIDDEN_LAYERS - 1):\n",
        "        start = end\n",
        "        end = end + NUM_HIDDEN\n",
        "        b = weightsAndBiases[start:end]\n",
        "        bs.append(b)\n",
        "\n",
        "    start = end\n",
        "    end = end + NUM_OUTPUT\n",
        "    b = weightsAndBiases[start:end]\n",
        "    bs.append(b)\n",
        "\n",
        "    return Ws, bs\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Qj9zKs3xz-"
      },
      "source": [
        "\r\n",
        "def training_validation_splits(X, Y, train_pct):\r\n",
        "    split_num = int(np.round(X.shape[0] * train_pct,0))\r\n",
        "    all_idx = random.permutation(X.shape[0])\r\n",
        "    tr_idx, te_idx = all_idx[:split_num], all_idx[split_num:]\r\n",
        "    X_tr, X_val = X[tr_idx,:], X[te_idx,:]\r\n",
        "    Y_tr, Y_val = Y[tr_idx], Y[te_idx]\r\n",
        "    return ([X_tr, X_val, Y_tr, Y_val])\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c6v2Icm3x8s"
      },
      "source": [
        "\r\n",
        "def randomize_dataset_order(X, Y):\r\n",
        "    all_idx = random.permutation(X.shape[0])\r\n",
        "    return ([X[all_idx], Y[all_idx]])\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FczrKQVSxi7c"
      },
      "source": [
        "\r\n",
        "def computeAccuracy (y, yhat):\r\n",
        "    yhat = yhat.T\r\n",
        "    return np.mean(np.argmax(y, axis=1) == np.argmax(yhat, axis=1))\r\n",
        "\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtBBZ0W1xiY-"
      },
      "source": [
        "\r\n",
        "def fCE (y, yhat, weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN, ALPHA = 0.):\r\n",
        "    Ws, bs = unpack_new(weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN)\r\n",
        "    y = y.T\r\n",
        "    L2_reg = ALPHA/y.shape[1] * np.sum([np.sum(W**2) for W in Ws])\r\n",
        "    cost = - 1./y.shape[1] * np.sum(y * np.log(yhat)) + L2_reg\r\n",
        "    return cost\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM5Mt63Scb2G"
      },
      "source": [
        "\r\n",
        "def softmax(z):\r\n",
        "    denom = np.sum(np.exp(z), axis=0, keepdims=True)\r\n",
        "    return np.exp(z) / denom\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD9KK8DT3zJ0"
      },
      "source": [
        "\r\n",
        "def ReLU(z):\r\n",
        "    return np.maximum(z,0)\r\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GykNtPBTEOPb"
      },
      "source": [
        "\r\n",
        "def ReLU_prime(z):\r\n",
        "    return np.where(z > 0, 1.0, 0.0)\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaIeJF8DcrLH"
      },
      "source": [
        "\r\n",
        "def forward_prop (x, y, weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN, ALPHA):\r\n",
        "\r\n",
        "    Ws, bs = unpack_new(weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN)\r\n",
        "\r\n",
        "    x = np.atleast_2d(x)\r\n",
        "    y = np.atleast_2d(y)\r\n",
        "\r\n",
        "    n = x.shape[0]\r\n",
        "    hs = []\r\n",
        "    zs = []\r\n",
        "\r\n",
        "    # set h0 equal to the input x.\r\n",
        "    hs.append(x.T)\r\n",
        "    # make dummy element for z[0] so indexing starts at 1\r\n",
        "    zs.append(x.T)  \r\n",
        "\r\n",
        "    l = NUM_HIDDEN_LAYERS + 1\r\n",
        "    for k in range(1, l):\r\n",
        "      b = np.atleast_2d(bs[k-1]).T\r\n",
        "      zs.append(Ws[k-1].dot(hs[k-1]) + b) \r\n",
        "      hs.append(ReLU(zs[k]))\r\n",
        "\r\n",
        "    # output layer\r\n",
        "    b = np.atleast_2d(bs[l-1]).T\r\n",
        "    zs.append(Ws[l-1].dot(hs[l-1]) + b)   \r\n",
        "    yhat = softmax(zs[l]) \r\n",
        "\r\n",
        "    loss = fCE(y, yhat, weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN, ALPHA)\r\n",
        "\r\n",
        "    # Return loss, pre-activations, post-activations, and predictions\r\n",
        "    return loss, zs, hs, yhat\r\n",
        "   "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4RXPKOBwE5b"
      },
      "source": [
        "\r\n",
        "def back_prop (x, y, weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN, ALPHA):  \r\n",
        "    # dimensions:             indexes:\r\n",
        "    # x: (m x n)\r\n",
        "    # y: (c x n)\r\n",
        "    # zs (outputs x n)        1 ... l\r\n",
        "    # hs (outputs x n)        1 ... l \r\n",
        "    # Ws (outputs x inputs)   0 ... l-1\r\n",
        "    # bs (outputs)            0 ... l-1\r\n",
        "\r\n",
        "    loss, zs, hs, yhat = forward_prop(x, y, weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN, ALPHA)\r\n",
        "    Ws, bs = unpack_new(weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN)\r\n",
        "\r\n",
        "    x = np.atleast_2d(x)\r\n",
        "    y = np.atleast_2d(y)\r\n",
        "    y = y.T\r\n",
        "\r\n",
        "    n = y.shape[1]\r\n",
        "    l = NUM_HIDDEN_LAYERS + 1\r\n",
        "\r\n",
        "    dJdWs = []  # Gradients w.r.t. weights\r\n",
        "    dJdbs = []  # Gradients w.r.t. biases\r\n",
        "\r\n",
        "    g = yhat - y\r\n",
        "\r\n",
        "    for k in range(l, 0, -1):\r\n",
        "      dJdWs.append(g.dot(hs[k-1].T)/n)  # + reg grad       \r\n",
        "      dJdbs.append(np.mean(g, axis=1))  \r\n",
        "      if (k > 1):\r\n",
        "        g = Ws[k-1].T.dot(g)\r\n",
        "        g = g * ReLU_prime(zs[k-1])\r\n",
        "\r\n",
        "    # reverse order\r\n",
        "    dJdWs = dJdWs[::-1]\r\n",
        "    dJdbs = dJdbs[::-1]\r\n",
        "\r\n",
        "    return (np.hstack([ dJdW.flatten() for dJdW in dJdWs ] + [ dJdb.flatten() for dJdb in dJdbs ]))\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJXmupHKwdHS"
      },
      "source": [
        "\r\n",
        "def plotSGDPath (trainX, trainY, trajectory):\r\n",
        "    # TODO: change this toy plot to show a 2-d projection of the weight space\r\n",
        "    # along with the associated loss (cross-entropy), plus a superimposed \r\n",
        "    # trajectory across the landscape that was traversed using SGD. Use\r\n",
        "    # sklearn.decomposition.PCA's fit_transform and inverse_transform methods.\r\n",
        "\r\n",
        "    def toyFunction (x1, x2):\r\n",
        "        return np.sin((2 * x1**2 - x2) / 10.)\r\n",
        "\r\n",
        "    fig = plt.figure()\r\n",
        "    ax = fig.gca(projection='3d')\r\n",
        "\r\n",
        "    # Compute the CE loss on a grid of points (corresonding to different w).\r\n",
        "    axis1 = np.arange(-np.pi, +np.pi, 0.05)  # Just an example\r\n",
        "    axis2 = np.arange(-np.pi, +np.pi, 0.05)  # Just an example\r\n",
        "    Xaxis, Yaxis = np.meshgrid(axis1, axis2)\r\n",
        "    Zaxis = np.zeros((len(axis1), len(axis2)))\r\n",
        "    for i in range(len(axis1)):\r\n",
        "        for j in range(len(axis2)):\r\n",
        "            Zaxis[i,j] = toyFunction(Xaxis[i,j], Yaxis[i,j])\r\n",
        "    ax.plot_surface(Xaxis, Yaxis, Zaxis, alpha=0.6)  # Keep alpha < 1 so we can see the scatter plot too.\r\n",
        "\r\n",
        "    # Now superimpose a scatter plot showing the weights during SGD.\r\n",
        "    Xaxis = 2*np.pi*np.random.random(8) - np.pi  # Just an example\r\n",
        "    Yaxis = 2*np.pi*np.random.random(8) - np.pi  # Just an example\r\n",
        "    Zaxis = toyFunction(Xaxis, Yaxis)\r\n",
        "    ax.scatter(Xaxis, Yaxis, Zaxis, color='r')\r\n",
        "\r\n",
        "    plt.show()\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFqlYb7rwE-I"
      },
      "source": [
        "\r\n",
        "def train (trainX, trainY, testX, testY, size=60000,\r\n",
        "           NUM_HIDDEN_LAYERS=3, NUM_HIDDEN=10, NUM_EPOCHS=1, BATCH_SIZE=1, ALPHA=0, LEARNING_RATE=1):\r\n",
        "\r\n",
        "    # use subset of data for faster testing\r\n",
        "    try: \r\n",
        "      trainX, trainY = randomize_dataset_order(trainX, trainY)\r\n",
        "      trainX = trainX[0:size]\r\n",
        "      trainY = trainY[0:size]\r\n",
        "    except:\r\n",
        "      pass\r\n",
        "\r\n",
        "    # make train/val/test splits (80% train vs val)\r\n",
        "    X_tr, X_val, Y_tr, Y_val  = training_validation_splits(trainX, trainY, .8)\r\n",
        "\r\n",
        "    trajectory = []\r\n",
        "    weightsAndBiases = initWeightsAndBiases_new(NUM_HIDDEN_LAYERS, NUM_HIDDEN) \r\n",
        "    l = NUM_HIDDEN_LAYERS + 1\r\n",
        "\r\n",
        "    print(\"\\nTraining -\",\r\n",
        "      \"  NUM_HIDDEN_LAYERS:\", NUM_HIDDEN_LAYERS, \r\n",
        "      \"  NUM_HIDDEN:\", NUM_HIDDEN,  \r\n",
        "      \"  NUM_EPOCHS:\",NUM_EPOCHS, \r\n",
        "      \"  BATCH_SIZE:\", BATCH_SIZE,  \r\n",
        "      \"  ALPHA:\", ALPHA, \r\n",
        "      \"  LEARNING_RATE:\", LEARNING_RATE, \r\n",
        "      \"  size:\", size)\r\n",
        "\r\n",
        "    for epoch in range(NUM_EPOCHS):\r\n",
        "      X_tr, Y_tr = randomize_dataset_order(X_tr, Y_tr)\r\n",
        "\r\n",
        "      for i in range(0, X_tr.shape[0], BATCH_SIZE): \r\n",
        "        # print(\"Cummulative batch:\", i)\r\n",
        "        grads = back_prop(X_tr, Y_tr, weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN, ALPHA)\r\n",
        "        dJdWs, dJdbs = unpack_new(grads, NUM_HIDDEN_LAYERS, NUM_HIDDEN)\r\n",
        "        Ws, bs = unpack_new(weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN)\r\n",
        "\r\n",
        "        # update weights and biases\r\n",
        "        for k in range(0, l):\r\n",
        "          Ws[k] = Ws[k] - LEARNING_RATE * dJdWs[k] \r\n",
        "          bs[k] = bs[k] - LEARNING_RATE * dJdbs[k] \r\n",
        "\r\n",
        "        weightsAndBiases = np.hstack([ W.flatten() for W in Ws ] + [ b.flatten() for b in bs ])\r\n",
        "        \r\n",
        "        # collect training loss for trajectory\r\n",
        "        loss_tr, zs, hs, yhat = forward_prop(X_val, Y_val, weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN, ALPHA)\r\n",
        "        trajectory.append([weightsAndBiases, loss_tr])\r\n",
        "        acc = computeAccuracy(Y_val, yhat)\r\n",
        "        \r\n",
        "      # print(\"epoch:\", epoch, \" loss:\", loss_tr, \" acc:\", acc)\r\n",
        "\r\n",
        "    # validation loss and accuracy\r\n",
        "    loss_val, zs, hs, yhat = forward_prop(X_val, Y_val, weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN, ALPHA)\r\n",
        "    acc = computeAccuracy(Y_val, yhat)\r\n",
        "\r\n",
        "    print(\"validation loss:\", loss_val, \" acc:\", acc)\r\n",
        "\r\n",
        "    return loss_val, acc, trajectory\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yei6rslN7gNQ"
      },
      "source": [
        "\r\n",
        "def test (X_tr, Y_tr, X_te, Y_te, \r\n",
        "          NUM_HIDDEN_LAYERS=3, NUM_HIDDEN=10, NUM_EPOCHS=1, BATCH_SIZE=1, ALPHA=0, LEARNING_RATE=1):\r\n",
        "\r\n",
        "    weightsAndBiases = initWeightsAndBiases_new(NUM_HIDDEN_LAYERS, NUM_HIDDEN) \r\n",
        "    l = NUM_HIDDEN_LAYERS + 1\r\n",
        "\r\n",
        "    for epoch in range(NUM_EPOCHS):\r\n",
        "      X_tr, Y_tr = randomize_dataset_order(X_tr, Y_tr)\r\n",
        "\r\n",
        "      for i in range(0, X_tr.shape[0], BATCH_SIZE): \r\n",
        "        # print(\"Cummulative batch:\", i)\r\n",
        "        grads = back_prop(X_tr, Y_tr, weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN, ALPHA)\r\n",
        "        dJdWs, dJdbs = unpack_new(grads, NUM_HIDDEN_LAYERS, NUM_HIDDEN)\r\n",
        "        Ws, bs = unpack_new(weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN)\r\n",
        "\r\n",
        "        # update weights and biases\r\n",
        "        for k in range(0, l):\r\n",
        "          Ws[k] = Ws[k] - LEARNING_RATE * dJdWs[k] \r\n",
        "          bs[k] = bs[k] - LEARNING_RATE * dJdbs[k] \r\n",
        "\r\n",
        "        weightsAndBiases = np.hstack([ W.flatten() for W in Ws ] + [ b.flatten() for b in bs ])\r\n",
        "        \r\n",
        "\r\n",
        "    # test loss and accuracy\r\n",
        "    loss, zs, hs, yhat = forward_prop(X_te, Y_te, weightsAndBiases, NUM_HIDDEN_LAYERS, NUM_HIDDEN, ALPHA)\r\n",
        "    acc = computeAccuracy(Y_te, yhat)\r\n",
        "\r\n",
        "    print(\"epoch:\", epoch, \" loss:\", loss, \" acc:\", acc)\r\n",
        "\r\n",
        "    return loss, acc\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50YJBaiBAs8M"
      },
      "source": [
        "def findBestHyperparameters(trainX, trainY, testX, testY,\r\n",
        "                            size=60000,  # number of data samples to include\r\n",
        "                            num_hidden_layers_list = [3],\r\n",
        "                            num_hidden_list = [10],\r\n",
        "                            num_epochs_list = [1],\r\n",
        "                            batch_size_list = [256],\r\n",
        "                            alpha_list = [0],\r\n",
        "                            learning_rate_list = [.001]):\r\n",
        "\r\n",
        "    tuning_log = []\r\n",
        "    current_iter = 0\r\n",
        "    max_iters = (len(num_hidden_layers_list) *\r\n",
        "                 len(num_hidden_list) *\r\n",
        "                 len(num_epochs_list) *\r\n",
        "                 len(batch_size_list) *\r\n",
        "                 len(alpha_list) *\r\n",
        "                 len(learning_rate_list))\r\n",
        "\r\n",
        "    for NUM_HIDDEN_LAYERS in num_hidden_layers_list:\r\n",
        "      for NUM_HIDDEN in num_hidden_list:\r\n",
        "        for NUM_EPOCHS in num_epochs_list:\r\n",
        "          for BATCH_SIZE in batch_size_list:\r\n",
        "            for ALPHA in alpha_list:\r\n",
        "              for LEARNING_RATE in learning_rate_list:\r\n",
        "\r\n",
        "                t0 = time.time()\r\n",
        "\r\n",
        "                loss, acc,  trajectory = train(trainX, trainY, testX, testY, size,\r\n",
        "                                               NUM_HIDDEN_LAYERS, NUM_HIDDEN, \r\n",
        "                                               NUM_EPOCHS, BATCH_SIZE, ALPHA, LEARNING_RATE)\r\n",
        "                \r\n",
        "                t1 = time.time()\r\n",
        "\r\n",
        "                print(\"Loss:\", loss,\r\n",
        "                      \"  acc:\", acc, \r\n",
        "                      \"  NUM_HIDDEN_LAYERS:\", NUM_HIDDEN_LAYERS, \r\n",
        "                      \"  NUM_HIDDEN:\", NUM_HIDDEN,  \r\n",
        "                      \"  NUM_EPOCHS:\",NUM_EPOCHS, \r\n",
        "                      \"  BATCH_SIZE:\", BATCH_SIZE,  \r\n",
        "                      \"  ALPHA:\", ALPHA, \r\n",
        "                      \"  LEARNING_RATE:\", LEARNING_RATE, \r\n",
        "                      \"  elapsed:\", t1-t0)\r\n",
        "\r\n",
        "                # if the current loss results are the best so far, record these hyperparameters\r\n",
        "                if (current_iter>0):  \r\n",
        "                  if (loss < best_params['loss']):\r\n",
        "                    best_params = {'NUM_HIDDEN_LAYERS': NUM_HIDDEN_LAYERS,\r\n",
        "                                  'NUM_HIDDEN': NUM_HIDDEN,\r\n",
        "                                  'NUM_EPOCHS': NUM_EPOCHS,\r\n",
        "                                  'BATCH_SIZE': BATCH_SIZE,\r\n",
        "                                  'ALPHA': ALPHA,\r\n",
        "                                  'LEARNING_RATE': LEARNING_RATE,\r\n",
        "                                  'loss': loss,\r\n",
        "                                  'accuracy': acc} \r\n",
        "                else:\r\n",
        "                  best_params =  {'NUM_HIDDEN_LAYERS': NUM_HIDDEN_LAYERS,\r\n",
        "                                  'NUM_HIDDEN': NUM_HIDDEN,\r\n",
        "                                  'NUM_EPOCHS': NUM_EPOCHS,\r\n",
        "                                  'BATCH_SIZE': BATCH_SIZE,\r\n",
        "                                  'ALPHA': ALPHA,\r\n",
        "                                  'LEARNING_RATE': LEARNING_RATE,\r\n",
        "                                  'loss': loss,\r\n",
        "                                  'accuracy': acc} \r\n",
        "\r\n",
        "                # log tuning results for after analysis\r\n",
        "                tuning_results = {'NUM_HIDDEN_LAYERS': NUM_HIDDEN_LAYERS,\r\n",
        "                                  'NUM_HIDDEN': NUM_HIDDEN,\r\n",
        "                                  'NUM_EPOCHS': NUM_EPOCHS,\r\n",
        "                                  'BATCH_SIZE': BATCH_SIZE,\r\n",
        "                                  'ALPHA': ALPHA,\r\n",
        "                                  'LEARNING_RATE': LEARNING_RATE,\r\n",
        "                                  'loss': loss,\r\n",
        "                                  'accuracy': acc}\r\n",
        "\r\n",
        "                tuning_log.append(tuning_results)\r\n",
        "                current_iter = current_iter + 1\r\n",
        "\r\n",
        "\r\n",
        "    #completed testing all hyperparameters - output summary results\r\n",
        "    print('\\n Hyperparameters tested:', \r\n",
        "          '\\n   num_hidden_layers_list:', num_hidden_layers_list,\r\n",
        "          '\\n   num_hidden_list:', num_hidden_list,\r\n",
        "          '\\n   num_epochs_list:', num_epochs_list,\r\n",
        "          '\\n   batch_size_list:', batch_size_list,\r\n",
        "          '\\n   alpha_list:', alpha_list,\r\n",
        "          '\\n   learning_rate:', learning_rate_list)\r\n",
        "\r\n",
        "    print('\\n best_params:', \r\n",
        "          '\\n   NUM_HIDDEN_LAYERS:', best_params['NUM_HIDDEN_LAYERS'],\r\n",
        "          '\\n   NUM_HIDDEN:', best_params['NUM_HIDDEN'],\r\n",
        "          '\\n   NUM_EPOCHS:', best_params['NUM_EPOCHS'],\r\n",
        "          '\\n   BATCH_SIZE:', best_params['BATCH_SIZE'],\r\n",
        "          '\\n   ALPHA:', best_params['ALPHA'],\r\n",
        "          '\\n   learning_rate:', best_params['LEARNING_RATE'],\r\n",
        "          '\\n   loss:', best_params['loss'],\r\n",
        "          '\\n   accuracy:', best_params['accuracy'])\r\n",
        "\r\n",
        "    return (tuning_log, best_params)\r\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDqm44-vwFDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde5735c-4903-44d8-b369-ece3f8ea299e"
      },
      "source": [
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    # load data\r\n",
        "    trainX, testX, trainY, testY = load_data()\r\n",
        "\r\n",
        "    # Y class one-hot encoding\r\n",
        "    trainY, testY = one_hot_encode_y(trainY, testY, 10)\r\n",
        "\r\n",
        "    # Normalize X pixel values\r\n",
        "    trainX = normalize_inputs(trainX, 255)\r\n",
        "    testX = normalize_inputs(testX, 255)\r\n",
        "\r\n",
        "    # Initialize weights and biases randomly\r\n",
        "    weightsAndBiases = initWeightsAndBiases_new(NUM_HIDDEN_LAYERS=3, NUM_HIDDEN=10)\r\n",
        "    \r\n",
        "    \r\n",
        "    # # # Perform gradient check on random training examples\r\n",
        "    # print(scipy.optimize.check_grad(lambda wab: forward_prop(np.atleast_2d(trainX[0:5,:]), np.atleast_2d(trainY[0:5,:]), wab)[0], \\\r\n",
        "    #                                 lambda wab: back_prop(np.atleast_2d(trainX[0:5,:]), np.atleast_2d(trainY[0:5,:]), wab), \\\r\n",
        "    #                                 weightsAndBiases))\r\n",
        "\r\n",
        "\r\n",
        "    print(scipy.optimize.check_grad(lambda wab: forward_prop(np.atleast_2d(trainX[0:5,:]), \r\n",
        "                                                            np.atleast_2d(trainY[0:5,:]), \r\n",
        "                                                            wab,\r\n",
        "                                                            NUM_HIDDEN_LAYERS=3, NUM_HIDDEN=10, ALPHA=0)[0],\r\n",
        "                                    lambda wab: back_prop(np.atleast_2d(trainX[0:5,:]), \r\n",
        "                                                          np.atleast_2d(trainY[0:5,:]), \r\n",
        "                                                          wab,\r\n",
        "                                                          NUM_HIDDEN_LAYERS=3, NUM_HIDDEN=10, ALPHA=0), \r\n",
        "                                    weightsAndBiases))\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "    # Plot the SGD trajectory\r\n",
        "    # plotSGDPath(trainX, trainY, ws)\r\n",
        "    pass"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.410416608067793e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QnQaKswQbjw"
      },
      "source": [
        "\r\n",
        "# results, best_params = findBestHyperparameters(trainX, trainY, testX, testY, size=2500,\r\n",
        "#                                               num_hidden_layers_list = [3,4,5],\r\n",
        "#                                               num_hidden_list = [10, 30],\r\n",
        "#                                               num_epochs_list = [10, 20],\r\n",
        "#                                               batch_size_list = [16, 64, 256],\r\n",
        "#                                               alpha_list = [0.01, .05, .1],\r\n",
        "#                                               learning_rate_list = [.001, .01, .1])    \r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A_C_1wKY6dh",
        "outputId": "480f1681-9bab-4dae-edc3-9296d76d4b03"
      },
      "source": [
        "\r\n",
        "findBestHyperparameters(trainX, trainY, testX, testY, size=60000,\r\n",
        "                        num_hidden_layers_list = [4],\r\n",
        "                        num_hidden_list = [30],\r\n",
        "                        num_epochs_list = [10],\r\n",
        "                        batch_size_list = [16],\r\n",
        "                        alpha_list = [.01],\r\n",
        "                        learning_rate_list = [.1])    \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training -   NUM_HIDDEN_LAYERS: 4   NUM_HIDDEN: 30   NUM_EPOCHS: 10   BATCH_SIZE: 16   ALPHA: 0.01   LEARNING_RATE: 0.1   size: 60000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}